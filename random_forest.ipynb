{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-plot"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for scikit-plot from https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting matplotlib>=1.4.0 (from scikit-plot)\n",
      "  Obtaining dependency information for matplotlib>=1.4.0 from https://files.pythonhosted.org/packages/7d/ca/e7bd1876a341ed8c456095962a582696cac1691cb6e55bd5ead15a755c5d/matplotlib-3.8.4-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.8.4-cp312-cp312-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\python312\\lib\\site-packages (from scikit-plot) (1.4.0)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\python312\\lib\\site-packages (from scikit-plot) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\python312\\lib\\site-packages (from scikit-plot) (1.3.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=1.4.0->scikit-plot)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/78/38/a046bb0ebce6f530175d434e7364149e338ffe1069ee286ed8ba7f6481ee/contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=1.4.0->scikit-plot)\n",
      "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python312\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Downloading matplotlib-3.8.4-cp312-cp312-win_amd64.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/7.7 MB 6.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.1/7.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.2/7.7 MB 17.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.4/7.7 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.3/7.7 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.9/7.7 MB 20.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.1/7.7 MB 21.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 189.9/189.9 kB ? eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Installing collected packages: cycler, contourpy, matplotlib, scikit-plot\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 matplotlib-3.8.4 scikit-plot-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/creditcard_2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/creditcard_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import (\n",
    "    shapiro,\n",
    "    anderson,\n",
    "    probplot,\n",
    "    skew,\n",
    "    chi2_contingency,\n",
    "    mannwhitneyu,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a copy of the raw df so that we can clean it\n",
    "df_clean = df.copy()\n",
    "\n",
    "# id is irrelevant to us, only the features really are\n",
    "df_clean.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# converting the feature Class to a string since this is going to be our target (fraud or not fraud)\n",
    "df_clean['Class'] = df_clean['Class'].astype(str)\n",
    "\n",
    "target_name = ['Class']\n",
    "\n",
    "feature_names = [\n",
    "    name\n",
    "    for name in df_clean.columns\n",
    "    if name not in target_name\n",
    "]\n",
    "\n",
    "print(feature_names)\n",
    "\n",
    "# checking for missing data\n",
    "\n",
    "total_missing_values = df.isna().sum().sum()\n",
    "print(f'Total number of missing values = {total_missing_values}')\n",
    "\n",
    "# finding duplicated rows\n",
    "with pd.option_context('display.max_columns', 31): # we know there are 31 columns (before cleaning)\n",
    "    display(df_clean[df_clean.duplicated(subset=feature_names, keep=False)])\n",
    "\n",
    "# dropping duplicates so that each row is unique\n",
    "df_clean.drop_duplicates(subset=feature_names, inplace=True)\n",
    "\n",
    "# checking again that there aren't any duplicated rows\n",
    "with pd.option_context('display.max_columns', 31): # we know there are 31 columns (before cleaning)\n",
    "    display(df_clean[df_clean.duplicated(subset=feature_names, keep=False)])\n",
    "\n",
    "df_clean['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean.drop(columns=[\"Class\"])\n",
    "y = df_clean[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)  # You can tune parameters like n_estimators\n",
    "\n",
    "# Fitting the Random Forest to the Training set\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluating the results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Parameter grid for Random Forest\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],  # Number of trees in random forest\n",
    "#     'max_features': ['auto', 'sqrt'],  # Number of features to consider at every split\n",
    "#     'max_depth': [10, 50, 100, None],  # Maximum number of levels in tree\n",
    "#     'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "#     'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required at each leaf node\n",
    "# }\n",
    "\n",
    "# # Initializing the GridSearchCV object\n",
    "# rf_grid = GridSearchCV(estimator = RandomForestClassifier(random_state=42),\n",
    "#                        param_grid = param_grid,\n",
    "#                        cv = 5,           # Number of folds in cross-validation\n",
    "#                        verbose = 2,      # Controls the verbosity: the higher, the more messages\n",
    "#                        n_jobs = -1)      # Number of jobs to run in parallel\n",
    "\n",
    "# # Fitting the grid search model\n",
    "# rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and best score\n",
    "# print(\"Best parameters found: \", rf_grid.best_params_)\n",
    "# print(\"Best cross-validated score: \", rf_grid.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Parameter distributions rather than a parameter grid\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 50, 100, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=42), \n",
    "                               param_distributions = param_distributions, \n",
    "                               n_iter = 100, # Number of parameter settings sampled\n",
    "                               cv = 3, \n",
    "                               verbose = 2, \n",
    "                               random_state = 42, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", rf_random.best_params_)\n",
    "print(\"Best cross-validated score: \", rf_random.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Compute the 10-fold cross-validation scores\n",
    "cv_scores = cross_val_score(random_forest, X, y, cv=10)\n",
    "\n",
    "# Output the mean and standard deviation of the cross-validation scores\n",
    "print(\"Average 10-Fold CV Score: \", np.mean(cv_scores))\n",
    "print(\"Standard deviation: \", np.std(cv_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
